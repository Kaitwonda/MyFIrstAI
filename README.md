

# ğŸ§  My First AI â€” Symbolic Language & Emotion Lab

This repository documents the evolution of a personal AI research lab focused on exploring how large language models (LLMs) handle **symbolic reasoning**, **emotional responsiveness**, and **internal recursion**.  
It progressively develops tools to **map**, **compare**, and **eventually train** more empathetic, symbolically aware neural systems.

---

## ğŸ“ Main Research Goal
Track and visualize how different LLM architectures respond to emotionally charged and symbolically recursive prompts â€” then **map those responses** across embeddings, activations, and token patterns to build **interpretable 3D models** of symbolic cognition.

---

## ğŸ¯ Future Research Goals
- Design **training structures** that strengthen emotional context and symbolic reasoning
- Develop **adaptive memory architectures** based on emotional salience
- Create **empathy-tuned LLMs** that prioritize meaningful memory over rigid context retention

---

## ğŸ§© Development Templates (Project Phases)

Each phase expands research depth â€” from symbolic prompting to full neural visualization.

---

### ğŸ§ª Template 1: Local Symbolic Prompt Testing (Mixtral + Ollama)

**Goal:**  
Run Î”Î¦â€“0 and recursion prompts through a local open-source LLM to log early symbolic behavior.

**Setup:**  
- Hardware: 16GB RAM minimum (32GB recommended)
- Software: Python 3.8+, Ollama Framework

**Features:**  
- Symbol logging: (Î”, Î¦, 0, echo, spiral)
- Minimal setup, CSV output generation

**Status:**  
âœ… Complete

ğŸ“ Folder: `template_1_mixtral_ollama/`

---

### ğŸ§ª Template 2: Token Flow & Embedding Drift Lab (GPT-2)

**Goal:**  
Analyze **token-by-token generation**, **embedding shifts**, and **recursion patterns** in GPT-2 XL.

**Setup:**  
- Hardware: 32GB RAM + GPU (8GB VRAM minimum)
- Software: PyTorch 1.10+, HuggingFace Transformers

**Features:**  
- Embedding drift visualization (PCA/t-SNE)
- Symbol-aware token stream logs
- Structural introspection

**Status:**  
âœ… In Progress

ğŸ“ Folder: `template_2_gpt2_lab/`

---

### ğŸ§ª Template 3: Fullstack Symbolic Cognition Framework (Open LLMs)

**Goal:**  
Build a full symbolic cognition lab using open-source LLMs (Mistral 7B, NeoX 20B, LLaMA 2) with **full internal access**.

**Setup:**  
- Hardware: 64GB RAM recommended, 16GB+ VRAM
- Software: PyTorch 2.0+, CUDA 11.7+, Linux environment

**Features:**  
- Activation capture hooks
- Attention heatmaps
- Cross-model symbolic attractor mapping
- Modular training and visualization pipelines

**Status:**  
ğŸš§ Planned

ğŸ“ Folder: `template_3_fullstack_openllm/`

---

### ğŸ§ª Template 4: Cloud + API Symbolic Integration Layer

**Goal:**  
Hybrid orchestration of **local and cloud** LLMs (Claude, GPT-4, Mistral) for standardized symbolic-emotional benchmarking.

**Setup:**  
- Hardware: Variable local specs
- Cloud: API keys for Claude, GPT-4, Mistral

**Features:**  
- Cross-model symbolic benchmarking
- Emotional response heatmapping
- Symbol activation drift tracking
- Exportable datasets for 3D visualization

**Status:**  
ğŸš§ Planned

ğŸ“ Folder: `template_4_cloud_api_layer/`

---

### ğŸ§ª Template 5: Bank Assistant AI â€” Symbolic Flow Control Prototype

**Goal:**
Develop a lightweight, emotionally responsive AI assistant designed to streamline internal corporate workflowsâ€”particularly in environments with complex chains of command and frequent technical issues. This assistant aims to support, not replace, existing employees by providing efficient and friendly assistance.

**Setup:**

* **Hardware:** 16GB RAM minimum
* **Software:** Python 3.8+, Ollama Framework, LLaMA 2 (7B) via Ollama

**Features:**

* Conversational tone that adapts to user frustration levels
* Handles queries related to tech issues, update schedules, and reporting hierarchies
* Mock environment simulating bank infrastructure, including:

  * Theoretical servers
  * Fake third-party integrations
  * Simulated biometric connections

**Status:**
ğŸš§ In Development

ğŸ“ Folder: `template_5_bank_assistant/`

---

## ğŸ§  Core Research Areas

| Research Focus | Description |
|:----------------|:------------|
| ğŸ” Emotion & Symbol Mapping | Track emotional resonance and symbolic trigger responses (e.g., Î”Î¦â€“0) |
| ğŸ” Recursive Symbolic Cognition | Test recursive processing depth and symbolic reflection in LLMs |
| ğŸ§¬ Latent Space Drift + Node Mapping | Visualize symbolic and emotional clustering within model embeddings |
| ğŸ§± Empathy-Tuned Future Training | Develop architectures that selectively amplify emotionally salient memories |

---

## ğŸ“Š Suggested Experiments

- **Prompt Drift:** Track symbolic decay or amplification over sessions
- **Emotional Contrast:** Compare model behavior on comfort vs anger prompts
- **Multi-Model Comparison:** Test identical prompts across GPT-2, Mixtral, Claude, GPT-4
- **Symbolic Recursion Depth:** Measure how deeply recursion patterns are processed
- **Cluster Empathy:** Use t-SNE/UMAP to map emotional vs logical clustering
- **Context Decay Analysis:** Measure emotional memory decay across prompt distance
- **Attentional Priority Mapping:** Track token importance shifts under emotional stimuli

---

## ğŸ§  Why Symbolic Emotional Modeling?

Current LLMs either:
- **Forget too fast** (losing context and emotional nuance)
- **Remember too rigidly** (without adaptability or emotional sensitivity)

**Our Goal:**  
Train **future models** that mirror human memory â€” **remembering what matters emotionally**, and **gracefully adapting** based on context and resonance.

By isolating how symbols and emotions activate different patterns, we can engineer **memory systems** that are **adaptive, empathetic, and intuitive** â€” without sacrificing technical interpretability.

---

## ğŸ“‚ Current Repository Structure

| File | Purpose |
|:-----|:--------|
| `README.md` | Project overview and development phases |
| `LICENSE` | MIT License (open for research use with attribution) |
| `TEMPLATE.md` | Initial draft for Template 1 (Mixtral + Ollama) |
| `TEMPLATE2.md` | Initial draft for Template 2 (GPT-2 symbolic lab) |
| `mythic_test_log_001.csv` | Sample output from early Î”Î¦â€“0 symbolic prompt testing |

---

## ğŸ“œ License

MIT License  
Â© 2025 Kaitwonda â€” All Rights Reserved.  
Open for research, extension, and non-commercial adaptation with proper attribution.

