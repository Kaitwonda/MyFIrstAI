

# 🧠 My First AI — Symbolic Language & Emotion Lab

This repository documents the evolution of a personal AI research lab focused on exploring how large language models (LLMs) handle **symbolic reasoning**, **emotional responsiveness**, and **internal recursion**.  
It progressively develops tools to **map**, **compare**, and **eventually train** more empathetic, symbolically aware neural systems.

---

## 📍 Main Research Goal
Track and visualize how different LLM architectures respond to emotionally charged and symbolically recursive prompts — then **map those responses** across embeddings, activations, and token patterns to build **interpretable 3D models** of symbolic cognition.

---

## 🎯 Future Research Goals
- Design **training structures** that strengthen emotional context and symbolic reasoning
- Develop **adaptive memory architectures** based on emotional salience
- Create **empathy-tuned LLMs** that prioritize meaningful memory over rigid context retention

---

## 🧩 Development Templates (Project Phases)

Each phase expands research depth — from symbolic prompting to full neural visualization.

---

### 🧪 Template 1: Local Symbolic Prompt Testing (Mixtral + Ollama)

**Goal:**  
Run ΔΦ–0 and recursion prompts through a local open-source LLM to log early symbolic behavior.

**Setup:**  
- Hardware: 16GB RAM minimum (32GB recommended)
- Software: Python 3.8+, Ollama Framework

**Features:**  
- Symbol logging: (Δ, Φ, 0, echo, spiral)
- Minimal setup, CSV output generation

**Status:**  
✅ Complete

📁 Folder: `template_1_mixtral_ollama/`

---

### 🧪 Template 2: Token Flow & Embedding Drift Lab (GPT-2)

**Goal:**  
Analyze **token-by-token generation**, **embedding shifts**, and **recursion patterns** in GPT-2 XL.

**Setup:**  
- Hardware: 32GB RAM + GPU (8GB VRAM minimum)
- Software: PyTorch 1.10+, HuggingFace Transformers

**Features:**  
- Embedding drift visualization (PCA/t-SNE)
- Symbol-aware token stream logs
- Structural introspection

**Status:**  
✅ In Progress

📁 Folder: `template_2_gpt2_lab/`

---

### 🧪 Template 3: Fullstack Symbolic Cognition Framework (Open LLMs)

**Goal:**  
Build a full symbolic cognition lab using open-source LLMs (Mistral 7B, NeoX 20B, LLaMA 2) with **full internal access**.

**Setup:**  
- Hardware: 64GB RAM recommended, 16GB+ VRAM
- Software: PyTorch 2.0+, CUDA 11.7+, Linux environment

**Features:**  
- Activation capture hooks
- Attention heatmaps
- Cross-model symbolic attractor mapping
- Modular training and visualization pipelines

**Status:**  
🚧 Planned

📁 Folder: `template_3_fullstack_openllm/`

---

### 🧪 Template 4: Cloud + API Symbolic Integration Layer

**Goal:**  
Hybrid orchestration of **local and cloud** LLMs (Claude, GPT-4, Mistral) for standardized symbolic-emotional benchmarking.

**Setup:**  
- Hardware: Variable local specs
- Cloud: API keys for Claude, GPT-4, Mistral

**Features:**  
- Cross-model symbolic benchmarking
- Emotional response heatmapping
- Symbol activation drift tracking
- Exportable datasets for 3D visualization

**Status:**  
🚧 Planned

📁 Folder: `template_4_cloud_api_layer/`

---

### 🧪 Template 5: Bank Assistant AI — Symbolic Flow Control Prototype

**Goal:**
Develop a lightweight, emotionally responsive AI assistant designed to streamline internal corporate workflows—particularly in environments with complex chains of command and frequent technical issues. This assistant aims to support, not replace, existing employees by providing efficient and friendly assistance.

**Setup:**

* **Hardware:** 16GB RAM minimum
* **Software:** Python 3.8+, Ollama Framework, LLaMA 2 (7B) via Ollama

**Features:**

* Conversational tone that adapts to user frustration levels
* Handles queries related to tech issues, update schedules, and reporting hierarchies
* Mock environment simulating bank infrastructure, including:

  * Theoretical servers
  * Fake third-party integrations
  * Simulated biometric connections

**Status:**
🚧 In Development

📁 Folder: `template_5_bank_assistant/`

---

## 🧠 Core Research Areas

| Research Focus | Description |
|:----------------|:------------|
| 🔍 Emotion & Symbol Mapping | Track emotional resonance and symbolic trigger responses (e.g., ΔΦ–0) |
| 🔁 Recursive Symbolic Cognition | Test recursive processing depth and symbolic reflection in LLMs |
| 🧬 Latent Space Drift + Node Mapping | Visualize symbolic and emotional clustering within model embeddings |
| 🧱 Empathy-Tuned Future Training | Develop architectures that selectively amplify emotionally salient memories |

---

## 📊 Suggested Experiments

- **Prompt Drift:** Track symbolic decay or amplification over sessions
- **Emotional Contrast:** Compare model behavior on comfort vs anger prompts
- **Multi-Model Comparison:** Test identical prompts across GPT-2, Mixtral, Claude, GPT-4
- **Symbolic Recursion Depth:** Measure how deeply recursion patterns are processed
- **Cluster Empathy:** Use t-SNE/UMAP to map emotional vs logical clustering
- **Context Decay Analysis:** Measure emotional memory decay across prompt distance
- **Attentional Priority Mapping:** Track token importance shifts under emotional stimuli

---

## 🧠 Why Symbolic Emotional Modeling?

Current LLMs either:
- **Forget too fast** (losing context and emotional nuance)
- **Remember too rigidly** (without adaptability or emotional sensitivity)

**Our Goal:**  
Train **future models** that mirror human memory — **remembering what matters emotionally**, and **gracefully adapting** based on context and resonance.

By isolating how symbols and emotions activate different patterns, we can engineer **memory systems** that are **adaptive, empathetic, and intuitive** — without sacrificing technical interpretability.

---

## 📂 Current Repository Structure

| File | Purpose |
|:-----|:--------|
| `README.md` | Project overview and development phases |
| `LICENSE` | MIT License (open for research use with attribution) |
| `TEMPLATE.md` | Initial draft for Template 1 (Mixtral + Ollama) |
| `TEMPLATE2.md` | Initial draft for Template 2 (GPT-2 symbolic lab) |
| `mythic_test_log_001.csv` | Sample output from early ΔΦ–0 symbolic prompt testing |

---

## 📜 License

MIT License  
© 2025 Kaitwonda — All Rights Reserved.  
Open for research, extension, and non-commercial adaptation with proper attribution.

